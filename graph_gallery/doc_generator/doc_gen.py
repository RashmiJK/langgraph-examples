import os
import warnings

# Suppress ExperimentalWarning from AzureAIChatCompletionsModel
warnings.filterwarnings("ignore", message=".*AzureAIChatCompletionsModel is currently in preview.*")
from typing import TypedDict
from pydantic import BaseModel, Field
from typing import Literal
import json
from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel
from langgraph.graph import StateGraph, START, END
from graph_gallery.doc_generator.doc_gen_prompts import (
    PROMPT_FOR_OUTLINE_GENERATION, 
    PROMPT_FOR_OUTLINE_VALIDATION, 
    PROMPT_FOR_DOCUMENT_GENERATION
)


class State(TypedDict):
    """
    Represents the state of the workflow
    """    
    topic: str                      # User input 
    outline: str | None             # Generated by AI
    outline_validation: str | None  # Validated by AI; "VALID" or "INVALID - <reason>"
    document: str | None            # Generated by AI
    error_message: str | None       # To store potential errors or reasons for stopping the workflow
    

class DocGen:
    """
    DocGen class for generating documentation on a topic prompted by the user.
    """
    def __init__(self) -> None:
        """
        Initialize the DocGen class.
        """
        self._gllm_41 =  AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1", # Highest reasoning and accuracy.
            api_version="2024-08-01-preview"
        )
        self._gllm_41_mini = AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1-mini", # Balanced reasoning and accuracy
            api_version="2024-08-01-preview"
        )
        self._gllm_41_nano = AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1-nano", # Lower reasoning and accuracy
            api_version="2024-08-01-preview"
        )
        self._graph = self._build_workflow()


    def _build_workflow(self):
        """
        Build the workflow for the document generation.
        """
        # Build the graph
        workflow_builder = StateGraph(State)

        # Add nodes
        workflow_builder.add_node("draft_outline", self._draft_outline)
        workflow_builder.add_node("validate_outline", self._validate_outline)
        workflow_builder.add_node("generate_document", self._generate_document)

        # Add edges and conditional edges
        workflow_builder.add_edge(START, "draft_outline")
        workflow_builder.add_edge("draft_outline", "validate_outline")
        workflow_builder.add_conditional_edges("validate_outline", self._should_generate_document)
        workflow_builder.add_edge("generate_document", END)

        # Compile the graph
        graph = workflow_builder.compile()

        print(graph.get_graph().draw_ascii())

        return graph


    def _draft_outline(self, state: State) -> State:
        """
        Generate an outline for the topic prompted by the user.
        """

        topic = state.get("topic","")
        if not topic:
            return {"error_message": "Topic is required"}

        outline_chain = PROMPT_FOR_OUTLINE_GENERATION | self._gllm_41
        outline = outline_chain.invoke({"topic": topic})

        return {"outline": outline.content}


    def _validate_outline(self, state: State) -> State:
        """
        Validate the outline generated by the AI.
        """
        if state.get("error_message"):
            return {"outline_validation": "INVALID  - " + state.get("error_message")}

        outline = state.get("outline", "")
        if not outline:
            return {
                "outline_validation": "INVALID - Outline is empty",
                "error_message" : "Outline generation failed"
            }

        # schema for outline validation
        class OutlineValidationResponse(BaseModel):
            """
            Schema for outline validation response.
            """
            is_outline_valid: bool = Field(description="True if the outline strictly follows the required structure: Introduction, three distinct Main Points, and a Conclusion. False otherwise.")
            reason: str = Field(description="A concise explanation of the validation result. If invalid, specify which structural element is missing or incorrect.")

        validator_llm = self._gllm_41.with_structured_output(OutlineValidationResponse)    
        validator_chain = PROMPT_FOR_OUTLINE_GENERATION | validator_llm

        try:
            validator_response : OutlineValidationResponse = validator_chain.invoke({"outline": outline, "topic": state["topic"]})
            print("Validator response: ", json.dumps(validator_response.dict(), indent=4))
            if validator_response.is_outline_valid:
                return {"outline_validation": "VALID"}
            else:
                return {
                    "outline_validation": "INVALID - " + validator_response.reason,
                    "error_message" : "Outline validation failed"
                }

        except Exception as e:
            return {
                "outline_validation": "INVALID - Exception occured - " + str(e),
                "error_message" : "Outline validation failed" 
            }

    def _should_generate_document(self, state: State) -> Literal["generate_document", END]:
        """
        Determine whether to generate the document or end the workflow.
        """

        if state.get("outline_validation", "") == "VALID":
            return "generate_document"
        else:
            return END

    def _generate_document(self, state : State) -> State:
        """
        Generate the document based on the outline.
        """
        if state.get("error_message"):
            return {"document": "Document generation failed" + state.get("error_message")}        

        outline = state["outline"]  # .get is not used here as outline is guaranteed to be present
        topic = state["topic"]      

        doc_chain = PROMPT_FOR_DOCUMENT_GENERATION | self._gllm_41
        document = doc_chain.invoke({"outline": outline, "topic": topic})
        
        return {"document": document.content}   

    def respond(self, input: str) -> str:
        """
        Respond to the user input.
        """
        response = self._graph.invoke({"topic": input})
        print("Response: ", json.dumps(response, indent=4))
        return response.get("document", "")

        