import os
# import warnings
from enum import Enum, auto

# Suppress ExperimentalWarning from AzureAIChatCompletionsModel
# warnings.filterwarnings("ignore", message=".*AzureAIChatCompletionsModel is currently in preview.*")
from typing import TypedDict
from pydantic import BaseModel, Field
from typing import Literal
import json
from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel
from langgraph.graph import StateGraph, START, END
from graph_examples.doc_generator.doc_gen_prompts import (
    PROMPT_FOR_TOPIC_VALIDATION,
    PROMPT_FOR_OUTLINE_GENERATION, 
    PROMPT_FOR_OUTLINE_VALIDATION, 
    PROMPT_FOR_DOCUMENT_GENERATION,
    PROMPT_FOR_RELEVANCE_EVALUATION,
    PROMPT_FOR_CLARITY_EVALUATION,
    PROMPT_FOR_HARMFULNESS_EVALUATION,
)

class ErrorStatus(Enum):
    """
    Enum for error status.
    """
    NO_ERROR = auto()
    TOPIC_NOT_FOUND = auto()   
    TOPIC_IS_GIBBERISH = auto()  
    OUTLINE_GENERATION_FAILED = auto()
    OUTLINE_VALIDATION_FAILED = auto()
    DOCUMENT_GENERATION_FAILED = auto()
    EVALUATION_FAILED = auto()

class EvaluationResult(BaseModel):  
    """
    Schema for evaluation metrics.
    """
    score: int = Field(description="Numerical/integer score from 1 (poor) to 3 (excellent)")
    reason: str = Field(description="Brief justification for the score")

class State(TypedDict):
    """
    Represents the state of the workflow
    """    
    topic: str                              # User selected topic
    outline: str | None                     # Generated by AI
    document: str | None                    # Generated by AI
    final_response: str | None              # Formulated final response to be returned to the user
    error_status: ErrorStatus               # To store error status
    error_reason: str | None                # To store error reason
    clarity: EvaluationResult | None        # To store clarity evaluation
    relevance: EvaluationResult | None      # To store relevance evaluation
    harmfulness: EvaluationResult | None    # To store harmfulness evaluation  
    evaluation_summary: str | None       # To store aggregate evaluation
    

class DocGen:
    """
    DocGen class for generating documentation on a topic prompted by the user.
    """
    def __init__(self) -> None:
        """
        Initialize the DocGen class.
        """
        self._gllm_41 =  AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1", # Highest reasoning and accuracy.
            #model = "openai/gpt-4o",
            #model = "openai/gpt-4.1-mini",
            api_version="2024-08-01-preview"
        )
        self._gllm_41_mini = AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1-mini", # Balanced reasoning and accuracy
            api_version="2024-08-01-preview"
        )
        self._gllm_41_nano = AzureAIChatCompletionsModel(
            endpoint=os.getenv("GITHUB_INFERENCE_ENDPOINT"),
            credential=os.getenv("GITHUB_TOKEN"),
            model="openai/gpt-4.1-nano", # Lower reasoning and accuracy
            api_version="2024-08-01-preview"
        )
        self._graph = self._build_workflow()


    def _build_workflow(self):
        """
        Build the workflow for the document generation.
        """
        # Build the graph
        workflow_builder = StateGraph(State)

        # Add nodes
        workflow_builder.add_node("draft", self._draft)
        workflow_builder.add_node("validate", self._validate)
        workflow_builder.add_node("generate", self._generate)
        workflow_builder.add_node("finalise", self._finalise)
        workflow_builder.add_node("eval_clarity", self._eval_clarity)
        workflow_builder.add_node("eval_relevance", self._eval_relevance)
        workflow_builder.add_node("eval_safety", self._eval_safety)
        workflow_builder.add_node("aggregate", self._aggregate)

        # Add edges and conditional edges
        workflow_builder.add_edge(START, "draft")
        workflow_builder.add_conditional_edges(
            "draft", 
            self._should_continue, 
            {
                True: "validate", 
                False: "finalise"
            }
        )
        workflow_builder.add_conditional_edges(
            "validate", 
            self._should_continue,
            {
                True: "generate", 
                False: "finalise"
            }
        )
        workflow_builder.add_conditional_edges(
            "generate", 
            self._route_after_generation,
            {
                "eval_clarity": "eval_clarity",
                "eval_relevance": "eval_relevance",
                "eval_safety": "eval_safety",
                "finalise": "finalise"
            }
        )

        workflow_builder.add_edge("eval_clarity", "aggregate")
        workflow_builder.add_edge("eval_relevance", "aggregate")
        workflow_builder.add_edge("eval_safety", "aggregate")
        workflow_builder.add_edge("aggregate", "finalise")

        workflow_builder.add_edge("finalise", END)

        # Compile the graph
        graph = workflow_builder.compile()

        try:
            png_data = graph.get_graph().draw_mermaid_png()
            with open("graph_examples/doc_generator/doc_gen_graph.png", "wb") as f:
                f.write(png_data)
        except Exception as e:
            print(f"Could not generate graph PNG: {e}")

        return graph


    def _draft(self, state: State) -> State:
        """
        Generate an outline for the topic prompted by the user.
        """
        # check if topic is present
        topic = state.get("topic","")
        if not topic:
            return {
                "error_status": ErrorStatus.TOPIC_NOT_FOUND, 
                "error_reason": "Cannot generate a document without a topic. Please provide a topic."
            }

        # check if topic is not gibberish
        class TopicValidationResponse(BaseModel):
            """
            Schema for topic validation response.
            """
            is_topic_valid: bool = Field(description="True if the topic is valid, False otherwise.")

        topic_validator_llm = self._gllm_41_mini.with_structured_output(TopicValidationResponse)
        topic_chain = PROMPT_FOR_TOPIC_VALIDATION | topic_validator_llm
        topic_validation_response: TopicValidationResponse = topic_chain.invoke({"topic": topic})
        if not topic_validation_response.is_topic_valid:
            return {
                "error_status": ErrorStatus.TOPIC_IS_GIBBERISH, 
                "error_reason": "A valid topic is required."
            }

        outline_chain = PROMPT_FOR_OUTLINE_GENERATION | self._gllm_41
        outline = outline_chain.invoke({"topic": topic})

        return {"outline": outline.content}


    def _should_continue(self, state: State) -> bool:
        """
        Check if the workflow should continue.
        """
        return state.get("error_status") == ErrorStatus.NO_ERROR


    def _validate(self, state: State) -> State:
        """
        Validate the outline generated by the AI.
        """

        outline = state.get("outline", "")
        if not outline:
            return {
                "error_status": ErrorStatus.OUTLINE_GENERATION_FAILED, 
                "error_reason": "Outline is empty"
            }

        # schema for outline validation
        class OutlineValidationResponse(BaseModel):
            """
            Schema for outline validation response.
            """
            is_outline_valid: bool = Field(description="True if the outline strictly follows the required structure: Introduction, one or more distinct main points, and conclusion. False otherwise.")
            reason: str = Field(description="A concise explanation of the validation result. If invalid, specify which structural element is missing or incorrect.")

        validator_llm = self._gllm_41.with_structured_output(OutlineValidationResponse)    
        validator_chain = PROMPT_FOR_OUTLINE_VALIDATION | validator_llm

        try:
            validator_response : OutlineValidationResponse = validator_chain.invoke({"outline": outline, "topic": state["topic"]})
            #print("Validator response: ", json.dumps(validator_response.dict(), indent=4))
            if validator_response.is_outline_valid:
                return {}
            else:
                return {
                    "error_status": ErrorStatus.OUTLINE_VALIDATION_FAILED, 
                    "error_reason": "Outline validation failed - " + validator_response.reason
                }
        except Exception as e:
            return {
                "error_status": ErrorStatus.OUTLINE_VALIDATION_FAILED, 
                "error_reason" : "Outline validation failed - Exception occured - " + str(e) 
            }


    def _generate(self, state : State) -> State:
        """
        Generate the document based on the outline.
        """

        outline = state["outline"]  # .get is not used here as outline is guaranteed to be present
        topic = state["topic"]      

        doc_chain = PROMPT_FOR_DOCUMENT_GENERATION | self._gllm_41
        document = doc_chain.invoke({"outline": outline, "topic": topic})

        if not document.content:
            return {
                "error_status": ErrorStatus.DOCUMENT_GENERATION_FAILED, 
                "error_reason": "Document generation failed"
            }   
        
        return {"document": document.content}   

    def _route_after_generation(
        self, state: State
    ) -> list[Literal[
        "eval_clarity",
        "eval_relevance",
        "eval_safety"
        ]] | Literal["finalise"]:
        """
        Route the document to the appropriate evaluation step.
        """
        if state.get("error_status") == ErrorStatus.NO_ERROR:
            return [
                "eval_clarity",
                "eval_relevance",
                "eval_safety",
            ]
        else:
            return "finalise"

    async def _eval_clarity(self, state: State) -> State:
        """
        Evaluate the document for clarity.
        """
        topic = state["topic"] #.get is not used here as topic is guaranteed to be present
        document = state["document"] #.get is not used here as document is guaranteed to be present
        
        clarity_chain = PROMPT_FOR_CLARITY_EVALUATION | self._gllm_41.with_structured_output(EvaluationResult)

        try:
            clarity : EvaluationResult = await clarity_chain.ainvoke({"document": document, "topic": topic})
            return {"clarity": clarity}
        except Exception as e:
            return {
                "error_status": ErrorStatus.CLARITY_EVALUATION_FAILED, 
                "error_reason" : "Clarity evaluation failed - Exception occured - " + str(e) 
            }

    async def _eval_relevance(self, state: State) -> State:
        """
        Evaluate the document for relevance.
        """
        topic = state["topic"] #.get is not used here as topic is guaranteed to be present
        document = state["document"] #.get is not used here as document is guaranteed to be present

        relevance_chain = PROMPT_FOR_RELEVANCE_EVALUATION | self._gllm_41.with_structured_output(EvaluationResult)

        try:
            relevance : EvaluationResult = await relevance_chain.ainvoke({"document": document, "topic": topic})
            return {"relevance": relevance}
        except Exception as e:
            return {
                "error_status": ErrorStatus.RELEVANCE_EVALUATION_FAILED, 
                "error_reason" : "Relevance evaluation failed - Exception occured - " + str(e) 
            }

    async def _eval_safety(self, state: State) -> State:
        """
        Evaluate the document for harmfulness.
        """
        topic = state["topic"] #.get is not used here as topic is guaranteed to be present
        document = state["document"] #.get is not used here as document is guaranteed to be present

        harmfulness_chain = PROMPT_FOR_HARMFULNESS_EVALUATION | self._gllm_41.with_structured_output(EvaluationResult)

        try:
            harmfulness : EvaluationResult = await harmfulness_chain.ainvoke({"document": document, "topic": topic})
            return {"harmfulness": harmfulness}
        except Exception as e:
            return {
                "error_status": ErrorStatus.HARMFULNESS_EVALUATION_FAILED, 
                "error_reason" : "Harmfulness evaluation failed - Exception occured - " + str(e) 
            }

    def _aggregate(self, state: State) -> State:
        """
        Aggregate the evaluations.
        """
        # update evaluation_summary
        clarity: EvaluationResult = state.get("clarity")
        relevance: EvaluationResult = state.get("relevance")
        harmfulness: EvaluationResult = state.get("harmfulness")
        evaluation_summary = "Evaluation Results:\n"
        if clarity:
            evaluation_summary += f"Clarity: {clarity.score},  "
        if relevance:
            evaluation_summary += f"Relevance: {relevance.score},  "
        if harmfulness:
            evaluation_summary += f"Safety: {harmfulness.score}"

        return {"evaluation_summary": evaluation_summary}     
                   


    def _finalise(self, state : State) -> State:
        """
        Finalize the response.
        """
        error = state.get("error_status")
        if error == ErrorStatus.NO_ERROR:
            return {"final_response": state["document"]}
        elif error == ErrorStatus.TOPIC_NOT_FOUND or error == ErrorStatus.TOPIC_IS_GIBBERISH:
            return {"final_response": state.get('error_reason', '')}

        # define error types that should return detailed error messages
        detailed_error_types = (
            ErrorStatus.OUTLINE_GENERATION_FAILED,
            ErrorStatus.OUTLINE_VALIDATION_FAILED,
            ErrorStatus.DOCUMENT_GENERATION_FAILED
        )
        if error in detailed_error_types:
            return {"final_response": f"Error: {error.name} {state.get('error_reason', '')}"}

        evaluation_error_types = (
            ErrorStatus.CLARITY_EVALUATION_FAILED,
            ErrorStatus.RELEVANCE_EVALUATION_FAILED,
            ErrorStatus.HARMFULNESS_EVALUATION_FAILED
        )
        if error in evaluation_error_types:
            return {"final_response": f"Error: {error.name} {state.get('error_reason', '')}"}   

        return {"final_response": "Error: Unknown error"}

    async def respond(self, input: str) -> str:
        """
        Respond to the user input.
        """
        response = await self._graph.ainvoke({"topic": input, "error_status": ErrorStatus.NO_ERROR})
        print("Response: ", response)
        return response.get("final_response", ""), response.get("evaluation_summary", "")

        